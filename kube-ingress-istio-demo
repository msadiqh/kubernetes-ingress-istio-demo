==========================================================================================================================================================================================
         Deploying microservices application on kubernetes cluster (https://github.com/msadiqh/kubernetes-ingress-istio.git)
==========================================================================================================================================================================================

1 Install cluster using kubeadm tool.
  reference: https://www.server-world.info/en/note?os=CentOS_5&p=kubernetes&f=3
  
	1.1 Install and Run Docker service on All Nodes
	
	1.2 Turn to Permissive or Disabled for SELinux (/etc/selinux/config)
	
	1.3 Stop and disable Firewalld and iptables
	
	1.4 Follow below steps after installation.
	
	    1.4.1 To start using your cluster, you need to run the following commands on master node:
                  mkdir -p $HOME/.kube
		  cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
		  chown $(id -u):$(id -g) $HOME/.kube/config
			  
	    1.4.2 Now deploy a pod network to the cluster.
	          Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
                  https://kubernetes.io/docs/concepts/cluster-administration/addons/
			  
==========================================================================================================================================================================================		      
==========================================================================================================================================================================================

2 Localhost setup (laptop)

	2.1 configure kubectl to remotely manage kubernetes cluster.
		
	    2.1.1 Create a folder kube (C:\kube) and copy the kubectl binary in this location and update the path variable. 
	          After configuring the path to kubctl binary, check the version to confirm (kubectl version --short).
	          reference: https://kubernetes.io/docs/tasks/tools/install-kubectl/
			  
            2.1.2 Copy .kube directory from kubernetes master to user home directory (C:\Users\sadiqh), this directory has the cluster details.
	          reference: https://kubernetes.io/docs/tasks/tools/install-kubectl/
	          [root@kubemaster .kube]# whoami && echo $PWD && ls
	          root
                  /root/.kube
                  cache  config  http-cache
        
	    2.1.3 After setting the environment to access the cluster remotely, verify with below commands.
		  c:\>kubectl version --short
		  Client Version: v1.16.0
                  Server Version: v1.16.2

	          c:\>kubectl get nodes
		  NAME         STATUS   ROLES    AGE   VERSION
		  kubemaster   Ready    master   10d   v1.16.2
                  kubenode01   Ready    <none>   10d   v1.16.2
                  kubenode02   Ready    <none>   10d   v1.16.2

	2.2 Configure helm client and tiller server component.
	
	    2.2.1 INSTALLING HELM: 
		  Create a folder helm (C:\helm) and copy the helm binary in this path and update the path variable.
		  After configuring the path to helm binary, check the version to confirm (helm version).
	          reference: https://get.helm.sh/helm-v3.0.0-rc.3-windows-amd64.zip
			  
	    2.2.2 INSTALLING TILLER: (yamls\tiller\tiller_sa_cluster_admin.yaml)
		  Tiller, the server portion of Helm, typically runs inside of your Kubernetes cluster.
		  The easiest way to install tiller into the cluster is simply to run helm init. 
	          This will validate that helm’s local environment is set up correctly (and set it up if necessary). 
		  Then it will connect to whatever cluster kubectl connects to by default. Once it connects, it will install tiller into the kube-system namespace.
		  Check tiller is running and create service account for tiller with cluster admin role.
		  
                  kubectl get pods --namespace kube-system 
                  kubectl apply -f yamls\tiller\tiller_sa_cluster_admin.yaml
			  
	    2.2.3 After setting the environment for HELM package manager, verify with below commands.
		  c:\>helm version
                  Client: &version.Version{SemVer:"v2.16.0", GitCommit:"e13bc94621d4ef666250cfbe534aaabf342a49bb", GitTreeState:"clean"}
                  Server: &version.Version{SemVer:"v2.16.0", GitCommit:"e13bc94621d4ef666250cfbe534aaabf342a49bb", GitTreeState:"clean"}
			  
	          Verify tiller pod is deployed on cluster.
	          c:\>kubectl get pods -n kube-system -o wide | findstr tiller
	          NAME                                 READY   STATUS    RESTARTS   AGE     IP             NODE         NOMINATED NODE   READINESS GATES
                  tiller-deploy-68cff9d9cb-9tn26       1/1     Running   6          5d10h   10.244.2.134   kubenode02   <none>           <none>
			  
==========================================================================================================================================================================================
==========================================================================================================================================================================================

3 Configure NFS server and kubenetes worker nodes to dynamically provision persistant volumes on kubernetes cluster.

  3.1 NFS server configuration on NFS server node (nfskubenode01) (Disable iptables and firewalld).
	
      3.1.1 Install NFS server package, enable and start nfs services.
            yum -y install nfs-utils
	    systemctl enable rpcbind
	    systemctl enable nfs-server
	    systemctl enable nfs-lock
	    systemctl start rpcbind
            systemctl start nfs-server
            systemctl start nfs-lock
			  			  
      3.1.2 Create a directory(/srv/nfs/kubedata) change ownership to nobody and add entry in exports file as below.
		
	    [root@nfskubenode01 ~]# mkdir -p /srv/nfs/kubedata
            [root@nfskubenode01 ~]# chown nobody: /srv/nfs/kubedata	
	    [root@nfskubenode01 ~]# cat /etc/exports
            /srv/nfs/kubedata       *(rw,sync,no_subtree_check,no_root_squash,no_all_squash,insecure)
            [root@nfskubenode01 ~]# exportfs -rav 
	    [root@nfskubenode01 ~]# exportfs -v
	    /srv/nfs/kubedata
                                  <world>(sync,wdelay,hide,no_subtree_check,sec=sys,rw,insecure,no_root_squash,no_all_squash)
							 							 
 3.2 NFS client configuration on kubernetes worker nodes (kubenode01 && kubenode02)
	
     3.2.1 Install NFS package, enable and start below nfs services.
           yum -y install nfs-utils
           systemctl enable rpcbind
           systemctl enable nfs-lock
	   systemctl start rpcbind
           systemctl start nfs-lock
			  
     3.2.2 Verify if the share is working on worker nodes as below.
	   [root@kubenode01 ~]# showmount -e 192.168.0.33
           Export list for 192.168.0.33:
           /srv/nfs/kubedata *
           [root@kubenode01 ~]# mount -t nfs 192.168.0.33:/srv/nfs/kubedata /mnt
           [root@kubenode01 ~]# df -h | grep mnt
           192.168.0.33:/srv/nfs/kubedata    35G  1.5G   36G   4% /mnt
           [root@kubenode01 ~]# umount /mnt
			  
==========================================================================================================================================================================================
==========================================================================================================================================================================================			  

4 Configure NFS client provisioner to dynamically create PVs from storage class, yamls are in /yamls/nfs-provisioner/
  
  4.1 Create a service account 'nfs-client-provisioner', ClusterRole,ClusterRoleBinding,Role and RoleBinding (rbac.yaml)
      
      4.2 Create a default Storage Class 'managed-nfs-storage' (default-sc.yaml)
	
      4.3 Create a deployment for NFS client provisioner (deployment.yaml)
	
      4.4 Deploy the yaml files as below.
	  kubectl create -f rbac.yaml -f default-sc.yaml -f deployment.yaml
          serviceaccount/nfs-client-provisioner created
	  clusterrole.rbac.authorization.k8s.io/nfs-client-provisioner-runner created
	  clusterrolebinding.rbac.authorization.k8s.io/run-nfs-client-provisioner created
	  role.rbac.authorization.k8s.io/leader-locking-nfs-client-provisioner created
	  rolebinding.rbac.authorization.k8s.io/leader-locking-nfs-client-provisioner created
          storageclass.storage.k8s.io/managed-nfs-storage created
	  deployment.apps/nfs-client-provisioner created
		
	  kubectl get sc
	  NAME                            PROVISIONER       AGE
	  managed-nfs-storage (default)   devops.com/nfs   3m8s  
	    
	  kubectl get all | findstr nfs
	  pod/nfs-client-provisioner-c44d859d-vx5jl   1/1     Running   0          6m4s
	  deployment.apps/nfs-client-provisioner   1/1     1            1           6m4s
	  replicaset.apps/nfs-client-provisioner-c44d859d   1         1         1       6m4s
		
==========================================================================================================================================================================================
==========================================================================================================================================================================================	

5 Configure MetalLB load-balancer for bare metal Kubernetes cluster, yamls are in yamls/metallb/
  reference:https://metallb.universe.tf/installation/

  5.1 To install MetalLB, apply the manifest:
      kubectl apply -f https://raw.githubusercontent.com/google/metallb/v0.8.3/manifests/metallb.yaml
		
  5.2 The installation manifest does not include a configuration file.MetalLB’s components will still start, but will remain idle until you define and deploy a configmap.
      kubectl apply -f metallb-config.yaml
		
  5.3 Verify the installation:
      kubectl get all -n metallb-system
		
==========================================================================================================================================================================================
==========================================================================================================================================================================================		

6 Configure kubernetes dashboard to manage cluster using GUI. yamls are in yamls/kubernetes-dashboard/sa_cluster_admin.yaml
  Reference:https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/
  
  6.1 Install kubernetes dashboard using kubectl 
      
      6.1.1 kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta6/aio/deploy/recommended.yaml
		      			  
      6.1.2 Create cluster admin service account to login to dashboard.
            kubectl apply -f sa_cluster_admin.yaml
			  
      6.1.3 Edit kubernetes-dashboard service and change service type to LoadBalancer.
	    kubectl edit svc kubernetes-dashboard -n kubernetes-dashboard
	    
	    Execute following command to get the token to login the dashboard.
	    kubectl -n kubernetes-dashboard describe secret $(kubectl -n kubernetes-dashboard get secret | grep admin-user | awk '{print $1}')
			  
	    Get the external IP assigned by metallb and add A record to DNS to access the kubernetes dashboard
	    kubectl get svc kubernetes-dashboard -n kubernetes-dashboard
	    https://kubernetes-dashboard.devops.com
		
==========================================================================================================================================================================================
==========================================================================================================================================================================================		
7 Configure NGINX Ingress Controller and ISTIO service mesh for Kubernetes cluster.

  7.1 Install nginx ingress controller using helm.
      helm search ingress
      helm install --name my-nginx stable/nginx-ingress
      kubectl get all | fndstr nginx(service is of type loadbalancer, check if IP got assigned by metallb) 
  
  7.2 Create A record DNS entry 'nginx-controller.devops.com' with external IP assigned by metallb 

  7.3 Installing ISTIO service mesh.
      Reference: https://istio.io/docs/setup/install/istioctl/
      
      Download the Istio release which includes installation files, samples, and the istioctl command line utility and add the istioctl client to your path. 
      curl -L https://istio.io/downloadIstio | sh -
      cd istio-1.4.0/bin
      mv istioctl /usr/local/bin
  
  7.4 Install the demo profile
      istioctl manifest apply --set profile=demo
      Verify the installation
      kubectl get svc -n istio-system
      
  7.5 Create A record DNS entry 'istio-ingressgateway.devops.com' with external IP assigned by metallb
  
  7.6 While ISTIO installation with demo profile, grafana and kiali addons were enabled by default.
  
      7.6.1 Setup grafana in istio service mesh.
            kubectl get svc -n istio-system and edit grafana service to 'type=Loadbalancer'
            Login to istio grafana dashboard by adding dns A record for external IP assigned by metallb.
            http://grafana-istio.devops.com
      
      7.6.2 Setup kiali in istio service mesh.
            kubectl get svc -n istio-system and edit kiali service to 'type=Loadbalancer'
	    Login to istio kiali dashboard by adding dns A record for external IP assigned by metallb.
	    http://kiali-istio.devops.com
	    
==========================================================================================================================================================================================
==========================================================================================================================================================================================
8 Configure cluster monitoring using prometheus and grafana.

  8.1 Install prometheus using helm.
  
      8.1.1 Before installing prometheus, lets set value for service type in values file and pass it as argument while installing the chart. 
	    helm inspect values stable/prometheus > C:\sadiqh\kubernetes\helm\prometheus.values
	    type: LoadBalancer
			  
      8.1.2 helm install --name my-prometheus stable/prometheus --values helm\prometheus.values --namespace prometheus
            kubectl get all -n prometheus (service is of type loadbalancer, check if IP got assigned by metallb)
			  
      8.1.3 During the prometheus install, it will create two Persistant volumes dynamically since we configured NFS storage backend and defined storageclass (managed-nfs-storage)
            kubectl get pvc -n prometheus
            kubectl get pv -n prometheus	
			  
      8.1.4 Login to prometheus dashboard by adding dns A record for external IP assigned by metallb.
            kubectl get svc -n prometheus 
	    http://prometheus.devops.com
			  
  8.2 Install grafana using helm.
  
      8.2.1 Before installing grafana, lets set values in values file and pass it as argument while installing the chart. 
            helm inspect values stable/grafana > C:\sadiqh\kubernetes\helm\grafana.values
	    type: LoadBalancer
	    adminPassword: grafana123
	    enabled: true
			  
      8.2.2 helm install --name my-grafana stable/grafana --values helm\grafana.values --namespace grafana
            kubectl get all -n grafana (service is of type loadbalancer, check if IP got assigned by metallb)
		
      8.2.3 During the grafana install, it will create one Persistant volume dynamically since we configured NFS storage backend and defined storageclass (managed-nfs-storage)
	    kubectl get pvc -n grafana
	    kubectl get pv -n grafana
			  
      8.2.4 Login to grafana dashboard by adding dns A record for external IP assigned by metallb and configure data sources to use prometheus (dashboard id: 8588)
            kubectl get svc -n grafana
	    http://grafana.devops.com
			  
==========================================================================================================================================================================================
==========================================================================================================================================================================================			  
9 Exposing cluster services to be accessed outside the cluster using nginx ingress controller,We will deploy applications in seperate namespace, not in default.

Hipster Shop: Cloud-Native Microservices Demo Application 
	
  9.1 kubectl create namespace hipster-ingress
      git clone https://github.com/GoogleCloudPlatform/microservices-demo.git
      cd release and kubectl apply -f kubernetes-manifests.yaml -n hipster-ingress
		
  9.2 create ingress resource to allow http traffic on port 80.(yamls\ingress\hipster-ingress.yaml)
      kubectl apply -f hipster-ingress.yaml -n hipster-ingress
      kubectl get ingress -n hipster-ingress
		
  9.3 Create CNAME  DNS entry for nginx controller in dns server to access the application. 
	
  9.4 Access the application http://hipster-ingress.devops.com
	
Bookinfo application: 

  9.5 kubectl create namespace bookinfo-ingress
      git clone https://github.com/istio/istio.git
      cd istio kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml -n bookinfo-ingress
		 
  9.6 create ingress resource to allow http traffic on port 80.(yamls\ingress\bookinfo-ingress.yaml)
      kubectl apply -f bookinfo-ingress.yaml -n bookinfo-ingress
      kubectl get ingress -n bookinfo-ingress
		 
  9.7 Create CNAME DNS entry for nginx controller in dns server to access the application. 
	
  9.8 Access the application http://bookinfo-ingress.devops.com/productpage
	
==========================================================================================================================================================================================
==========================================================================================================================================================================================
10 Exposing cluster services to be accessed outside the cluster using ISTIO service mesh,We will deploy applications in seperate namespace, not in default.
	
Bookinfo application: (yamls\istio)

  10.5 kubectl create namespace bookinfo-istio
       kubectl label namespace bookinfo-istio istio-injection=enabled
       kubectl apply -f bookinfo.yaml -n bookinfo-istio
		 
  10.6 create gateway, virtualservice and destinationrule for bookinfo app.
       kubectl apply -f bookinfo-gateway.yaml -n bookinfo-istio
       kubectl apply -f destination-rule-all.yaml -n bookinfo-istio
		 
  10.7 Create CNAME DNS entry for istio-ingressgateway in dns server to access the application.
	
  10.8 Access the application http://bookinfo-istio.devops.com/productpage
  
===========================================================================================================================================================================================
===========================================================================================================================================================================================
11 Example use cases of ISTIO:(reviews microservice versions: v1-blank, v2-black stars, v3-red stars)

   11.1 The reviews microservice in bookinfo app has three version(v1,v2,v3), lets see how to split the traffic among different versions of microservice.
        lets create a virtualservice to send all the traffic coming from productpage to reviews v2 version (all black stars)
        and deny to v1 and v3.
  
       kubectl apply -f virtual-service-all-v2.yaml -n bookinfo-istio
  
       Generate traffic to bookinfo application using below command and visualize the traffic flow in kiali dashboard.
  
       watch curl -s -o /dev/null http://bookinfo-istio.devops.com
       
   11.2 Route the traffic to a particular microservice based on the header value.
  


