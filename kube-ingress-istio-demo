==========================================================================================================================================================================================
         Deploying microservices application on kubernetes cluster (https://github.com/msadiqh/kubernetes-ingress-istio.git)
==========================================================================================================================================================================================

Table of content:

1  Install cluster using kubeadm tool.
2  Configure kubectl to remotely manage the cluster and setup helm to install packages on cluster.
3  Configure NFS server and kubenetes worker nodes to dynamically provision persistant volumes on kubernetes cluster.
4  Configure NFS client provisioner to dynamically create PVs from default storage class.
5  Configure MetalLB load-balancer for bare metal Kubernetes cluster.
6  Configure kubernetes dashboard to manage cluster using GUI.
7  Configure NGINX Ingress Controller and ISTIO for Kubernetes cluster.
8  Configure cluster monitoring using prometheus and grafana.
9  Exposing services to be accessed outside the cluster using nginx ingress controller.
10 Exposing cluster services to be accessed outside the cluster using ISTIO service mesh
11 Example use cases for ISTIO

===========================================================================================================================================================================================
===========================================================================================================================================================================================

1 Install cluster using kubeadm tool.
  reference: https://www.server-world.info/en/note?os=CentOS_5&p=kubernetes&f=3
  
	1.1 Install and Run Docker service on All Nodes
	
	1.2 Turn to Permissive or Disabled for SELinux (/etc/selinux/config)
	
	1.3 Stop and disable Firewalld and iptables
	
	1.4 Follow below steps after installation.
	
	    1.4.1 To start using your cluster, you need to run the following commands on master node:
                  mkdir -p $HOME/.kube
		  cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
		  chown $(id -u):$(id -g) $HOME/.kube/config
			  
	    1.4.2 Now deploy a pod network to the cluster.
	          Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
                  https://kubernetes.io/docs/concepts/cluster-administration/addons/
			  
==========================================================================================================================================================================================		      
==========================================================================================================================================================================================

2 Configure kubectl to remotely manage the cluster and setup helm to install packages on cluster.
  Localhost setup (laptop)

	2.1 configure kubectl to remotely manage kubernetes cluster.
		
	    2.1.1 Create a folder kube (C:\kube) and copy the kubectl binary in this location and update the path variable. 
	          After configuring the path to kubctl binary, check the version to confirm (kubectl version --short).
	          reference: https://kubernetes.io/docs/tasks/tools/install-kubectl/
			  
            2.1.2 Copy .kube directory from kubernetes master to user home directory (C:\Users\sadiqh), this directory has the cluster details.
	          reference: https://kubernetes.io/docs/tasks/tools/install-kubectl/
	          [root@kubemaster .kube]# whoami && echo $PWD && ls
	          root
                  /root/.kube
                  cache  config  http-cache
        
	    2.1.3 After setting the environment to access the cluster remotely, verify with below commands.
		  c:\>kubectl version --short
		  Client Version: v1.16.0
                  Server Version: v1.16.2

	          c:\>kubectl get nodes
		  NAME         STATUS   ROLES    AGE   VERSION
		  kubemaster   Ready    master   10d   v1.16.2
                  kubenode01   Ready    <none>   10d   v1.16.2
                  kubenode02   Ready    <none>   10d   v1.16.2

	2.2 Configure helm client and tiller server component.
	
	    2.2.1 INSTALLING HELM: 
		  Create a folder helm (C:\helm) and copy the helm binary in this path and update the path variable.
		  After configuring the path to helm binary, check the version to confirm (helm version).
	          reference: https://get.helm.sh/helm-v3.0.0-rc.3-windows-amd64.zip
			  
	    2.2.2 INSTALLING TILLER: (kubernetes-ingress-istio-demo\yamls\tiller\tiller_sa_cluster_admin.yaml)
		  Tiller, the server portion of Helm, typically runs inside of your Kubernetes cluster.
		  The easiest way to install tiller into the cluster is simply to run helm init. 
	          This will validate that helmâ€™s local environment is set up correctly (and set it up if necessary). 
		  Then it will connect to whatever cluster kubectl connects to by default. Once it connects, it will install tiller into the kube-system namespace.
		  Check tiller is running and create service account for tiller with cluster admin role.
		  
                  kubectl get pods --namespace kube-system 
                  kubectl apply -f tiller_sa_cluster_admin.yaml
			  
	    2.2.3 After setting the environment for HELM package manager, verify with below commands.
		  c:\>helm version
                  Client: &version.Version{SemVer:"v2.16.0", GitCommit:"e13bc94621d4ef666250cfbe534aaabf342a49bb", GitTreeState:"clean"}
                  Server: &version.Version{SemVer:"v2.16.0", GitCommit:"e13bc94621d4ef666250cfbe534aaabf342a49bb", GitTreeState:"clean"}
			  
	          Verify tiller pod is deployed on cluster.
	          c:\>kubectl get pods -n kube-system -o wide 
	          NAME                                 READY   STATUS    RESTARTS   AGE     IP             NODE         NOMINATED NODE   READINESS GATES
                  tiller-deploy-68cff9d9cb-9tn26       1/1     Running   6          5d10h   10.244.2.134   kubenode02   <none>           <none>
			  
==========================================================================================================================================================================================
==========================================================================================================================================================================================

3 Configure NFS server and kubenetes worker nodes to dynamically provision persistant volumes on kubernetes cluster.

  3.1 NFS server configuration on NFS server node (nfskubenode01) (Disable iptables and firewalld).
	
      3.1.1 Install NFS server package, enable and start nfs services.
            yum -y install nfs-utils
	    systemctl enable rpcbind
	    systemctl enable nfs-server
	    systemctl enable nfs-lock
	    systemctl start rpcbind
            systemctl start nfs-server
            systemctl start nfs-lock
			  			  
      3.1.2 Create a directory(/srv/nfs/kubedata) change ownership to nobody and add entry in exports file as below.
		
	    [root@nfskubenode01 ~]# mkdir -p /srv/nfs/kubedata
            [root@nfskubenode01 ~]# chown nobody: /srv/nfs/kubedata	
	    [root@nfskubenode01 ~]# cat /etc/exports
            /srv/nfs/kubedata       *(rw,sync,no_subtree_check,no_root_squash,no_all_squash,insecure)
            [root@nfskubenode01 ~]# exportfs -rav 
	    [root@nfskubenode01 ~]# exportfs -v
	    /srv/nfs/kubedata
                                  <world>(sync,wdelay,hide,no_subtree_check,sec=sys,rw,insecure,no_root_squash,no_all_squash)
							 							 
 3.2 NFS client configuration on kubernetes worker nodes (kubenode01 && kubenode02)
	
     3.2.1 Install NFS package, enable and start below nfs services.
           yum -y install nfs-utils
           systemctl enable rpcbind
           systemctl enable nfs-lock
	   systemctl start rpcbind
           systemctl start nfs-lock
			  
     3.2.2 Verify if the share is working on worker nodes as below.
	   [root@kubenode01 ~]# showmount -e 192.168.0.33
           Export list for 192.168.0.33:
           /srv/nfs/kubedata *
           [root@kubenode01 ~]# mount -t nfs 192.168.0.33:/srv/nfs/kubedata /mnt
           [root@kubenode01 ~]# df -h | grep mnt
           192.168.0.33:/srv/nfs/kubedata    35G  1.5G   36G   4% /mnt
           [root@kubenode01 ~]# umount /mnt
			  
==========================================================================================================================================================================================
==========================================================================================================================================================================================			  

4 Configure NFS client provisioner to dynamically create PVs from default storage class. (kubernetes-ingress-istio-demo/yamls/nfs-provisioner)
  
  4.1 Create a service account 'nfs-client-provisioner', ClusterRole,ClusterRoleBinding,Role and RoleBinding (rbac.yaml)
      
      4.2 Create a default Storage Class 'managed-nfs-storage' (default-sc.yaml)
	
      4.3 Create a deployment for NFS client provisioner (deployment.yaml)
	
      4.4 Deploy the yaml files as below.
	  kubectl create -f rbac.yaml -f default-sc.yaml -f deployment.yaml
          serviceaccount/nfs-client-provisioner created
	  clusterrole.rbac.authorization.k8s.io/nfs-client-provisioner-runner created
	  clusterrolebinding.rbac.authorization.k8s.io/run-nfs-client-provisioner created
	  role.rbac.authorization.k8s.io/leader-locking-nfs-client-provisioner created
	  rolebinding.rbac.authorization.k8s.io/leader-locking-nfs-client-provisioner created
          storageclass.storage.k8s.io/managed-nfs-storage created
	  deployment.apps/nfs-client-provisioner created
		
	  kubectl get sc
	  NAME                            PROVISIONER       AGE
	  managed-nfs-storage (default)   devops.com/nfs   3m8s  
	    
	  kubectl get all | findstr nfs
	  pod/nfs-client-provisioner-c44d859d-vx5jl   1/1     Running   0          6m4s
	  deployment.apps/nfs-client-provisioner   1/1     1            1           6m4s
	  replicaset.apps/nfs-client-provisioner-c44d859d   1         1         1       6m4s
		
==========================================================================================================================================================================================
==========================================================================================================================================================================================	

5 Configure MetalLB load-balancer for bare metal Kubernetes cluster. (kubernetes-ingress-istio-demo/yamls/metallb)
  reference:https://metallb.universe.tf/installation/

  5.1 To install MetalLB, apply the manifest:
      kubectl apply -f https://raw.githubusercontent.com/google/metallb/v0.8.3/manifests/metallb.yaml
		
  5.2 The installation manifest does not include a configuration file.MetalLBâ€™s components will still start, but will remain idle until you define and deploy a configmap.
      kubectl apply -f metallb-config.yaml
		
  5.3 Verify the installation:
      kubectl get all -n metallb-system
		
==========================================================================================================================================================================================
==========================================================================================================================================================================================		

6 Configure kubernetes dashboard to manage cluster using GUI. (kubernetes-ingress-istio-demo/yamls/kubernetes-dashboard/)
  Reference:https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/
  
  6.1 Install kubernetes dashboard using kubectl 
      
      6.1.1 kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta6/aio/deploy/recommended.yaml
		      			  
      6.1.2 Create cluster admin service account to login to dashboard.
            kubectl apply -f sa_cluster_admin.yaml
			  
      6.1.3 Edit kubernetes-dashboard service and change service type to LoadBalancer.
	    kubectl edit svc kubernetes-dashboard -n kubernetes-dashboard
	    
      6.1.4 Create A record DNS entry 'kubernetes-dashboard.devops.com' with external IP assigned by metallb
            kubectl get svc kubernetes-dashboard -n kubernetes-dashboard
	    
	    Execute following command to get the token to login the dashboard.
	    kubectl -n kubernetes-dashboard describe secret $(kubectl -n kubernetes-dashboard get secret | grep admin-user | awk '{print $1}')
			  
      6.1.5 Login to kubernetes dashboard with below url.
	    https://kubernetes-dashboard.devops.com
		
==========================================================================================================================================================================================
==========================================================================================================================================================================================		
7 Configure NGINX Ingress Controller and ISTIO service mesh for Kubernetes cluster.

  7.1 Install nginx ingress controller using helm.
      helm search ingress
      helm install --name helm-nginx stable/nginx-ingress
      kubectl get all | fndstr nginx(service is of type loadbalancer, check if IP got assigned by metallb) 
  
  7.2 Create A record DNS entry 'nginx-controller.devops.com' with external IP assigned by metallb 

  7.3 Installing ISTIO service mesh.
      Reference: https://istio.io/docs/setup/install/istioctl/
      
      Download the Istio release which includes installation files, samples, and the istioctl command line utility and add the istioctl client to your path. 
      curl -L https://istio.io/downloadIstio | sh -
      cd istio-1.4.0/bin
      mv istioctl /usr/local/bin
  
  7.4 Install the demo profile
      istioctl manifest apply --set profile=demo
      Verify the installation
      kubectl get svc -n istio-system
      
  7.5 Create A record DNS entry 'istio-ingressgateway.devops.com' with external IP assigned by metallb
  
  7.6 While ISTIO installation with demo profile, grafana and kiali addons were enabled by default.
  
      7.6.1 Setup grafana in istio service mesh.
            kubectl get svc -n istio-system and edit grafana service to 'type=Loadbalancer'
            Login to istio grafana dashboard by adding dns A record for external IP assigned by metallb.
            http://grafana-istio.devops.com:3000
	    
     7.6.2 Setup prometheus in istio service mesh.
           kubectl get svc -n istio-system and edit prometheus service to 'type=Loadbalancer'
	   Login to istio prometheus dashboard by adding dns A record for external IP assigned by metallb.
	   http://prometheus-istio.devops.com:9090
      
     7.6.3 Setup kiali in istio service mesh.
           kubectl get svc -n istio-system and edit kiali service to 'type=Loadbalancer'
	   Login to istio kiali dashboard by adding dns A record for external IP assigned by metallb.
	   http://kiali-istio.devops.com:20001
	    
==========================================================================================================================================================================================
==========================================================================================================================================================================================
8 Configure cluster monitoring using prometheus and grafana.(kubernetes-ingress-istio-demo/helm/)

  8.1 Install prometheus using helm.
  
      8.1.1 Before installing prometheus, lets set value for service type in values file and pass it as argument while installing the chart. 
	    helm inspect values stable/prometheus > C:\sadiqh\kubernetes\helm\prometheus.values
	    type: LoadBalancer
			  
      8.1.2 helm install --name helm-prometheus stable/prometheus --values prometheus.values --namespace prometheus
            kubectl get all -n prometheus (service is of type loadbalancer, check if IP got assigned by metallb)
			  
      8.1.3 During the prometheus install, it will create two Persistant volumes dynamically since we configured NFS storage backend and defined default storageclass (managed-nfs-storage)
            kubectl get pvc -n prometheus
            kubectl get pv -n prometheus	
			  
      8.1.4 Login to prometheus dashboard by adding dns A record for external IP assigned by metallb.
            kubectl get svc -n prometheus 
	    http://prometheus.devops.com
			  
  8.2 Install grafana using helm.
  
      8.2.1 Before installing grafana, lets set values in values file and pass it as argument while installing the chart. 
            helm inspect values stable/grafana > C:\sadiqh\kubernetes\helm\grafana.values
	    type: LoadBalancer
	    adminPassword: grafana123
	    enabled: true
			  
      8.2.2 helm install --name helm-grafana stable/grafana --values grafana.values --namespace grafana
            kubectl get all -n grafana (service is of type loadbalancer, check if IP got assigned by metallb)
		
      8.2.3 During the grafana install, it will create one Persistant volume dynamically since we configured NFS storage backend and defined default storageclass (managed-nfs-storage)
	    kubectl get pvc -n grafana
	    kubectl get pv -n grafana
			  
      8.2.4 Login to grafana dashboard by adding dns A record for external IP assigned by metallb and configure data sources to use prometheus (dashboard id: 8588)
            kubectl get svc -n grafana
	    http://grafana.devops.com
			  
==========================================================================================================================================================================================
==========================================================================================================================================================================================			  
9 Exposing cluster services to be accessed outside the cluster using nginx ingress controller. (kubernetes-ingress-istio-demo/yamls/ingress/)

Hipster Shop: Cloud-Native Microservices Demo Application 
	
  9.1 kubectl create namespace hipster-nginx
      git clone https://github.com/GoogleCloudPlatform/microservices-demo.git
      cd release and kubectl apply -f kubernetes-manifests.yaml -n hipster-nginx
		
  9.2 create ingress resource to allow http traffic on port 80.
      kubectl apply -f hipster-ingress.yaml 
      kubectl get ingress -n hipster-nginx
		
  9.3 Create CNAME (hipster-nginx.devops.com) DNS entry for nginx ingress controller in dns server to access the application. 
	
  9.4 Access the application http://hipster-nginx.devops.com
	
Bookinfo application: 

  9.5 kubectl create namespace bookinfo-nginx
      git clone https://github.com/istio/istio.git
      cd istio kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml -n bookinfo-nginx
      kubectl edit svc productpage -n bookinfo-nginx (change the service port 9080 to 80)
		 
  9.6 create ingress resource to allow http traffic on port 80.
      kubectl apply -f bookinfo-ingress.yaml 
      kubectl get ingress -n bookinfo-nginx
		 
  9.7 Create CNAME (bookinfo-nginx.devops.com) DNS entry for nginx ingress controller in dns server to access the application. 
	
  9.8 Access the application http://bookinfo-nginx.devops.com/
	
==========================================================================================================================================================================================
==========================================================================================================================================================================================
10 Exposing cluster services to be accessed outside the cluster using ISTIO service mesh. ((kubernetes-ingress-istio-demo/yamls/istio/)
	
Bookinfo application: 

  10.5 kubectl create namespace bookinfo-istio
       kubectl label namespace bookinfo-istio istio-injection=enabled
       kubectl apply -f bookinfo.yaml -n bookinfo-istio
		 
  10.6 create gateway, virtualservice and destinationrule for bookinfo app.
       kubectl apply -f bookinfo-gateway.yaml -n bookinfo-istio
       kubectl apply -f destination-rule-all.yaml -n bookinfo-istio
		 
  10.7 Create CNAME DNS entry for istio-ingressgateway in dns server to access the application.
	
  10.8 Access the application http://bookinfo-istio.devops.com/productpage
  
===========================================================================================================================================================================================
===========================================================================================================================================================================================
11 Example use cases for ISTIO:(reviews microservice versions: v1- no rating, v2-black stars, v3-red stars)
   (kubernetes-ingress-istio-demo/yamls/istio/usecase/)

Note:  Reviews microservice in bookinfo app has threee versions, where v1 does not call ratings service, v2 calls rating returns all black star rating
       and v3 calls rating service returns all red star rating.

   11.1 Request routing or traffic shifting to a particular version of a microservice. (blue-green deployment)
   
        Lets route traffic coming from product page to reviews v2 subset and deny v1 and v3.
	
	kubectl apply -f virtual-service-reviews-all-v2.yaml -n bookinfo-istio
	
	To verify check bookinfo app in browser and kiali to verify the traffic flow.
	
   11.2 Weighted routing between different versions of a microservice. (canary deployment)
        Lets route the traffic from productpage to reviews v2 and v3 subset, here we will split the traffic as below:
	90% ==> v2
	10% ==> v3
	
	kubectl apply -f virtual-service-reviews-weighted-v2-90-v3-10.yaml -n bookinfo-istio
        
        To verify check bookinfo app in browser and kiali to verify the traffic flow.
	
   11.3 Route based on user identity to a particular version of service. (A/B Testing)
   
        Lets route traffic from productpage service to reviews v2 if the http hearder for 'end-user' match the value 'sadiqh' or
	else route the traffic to v3 version of reviews service.
	
	kubectl apply -f virtual-service-reviews-sadiqh-v2.yaml -n bookinfo-istio
	
        To verify check the bookinfo app in browser, all traffic will route to reviews v3 service (red star rating) and route to reviews v2 service (black star rating) only when user 'sadiqh' logs in.
	
   11.4 Injecting an HTTP delay fault:
   
        11.4.1 Before injecting the http delay, lets configure routing rules as below.
	
	      kubectl apply -f virtual-service-all-v1.yaml -n bookinfo-istio
              kubectl apply -f virtual-service-reviews-test-v2.yaml -n bookinfo-istio
	
	      With the above configuration, this is how requests flow:

              productpage â†’ reviews:v2 â†’ ratings (only for user sadiqh)
              productpage â†’ reviews:v1 (for everyone else)
	      
	      Create a fault injection rule to delay traffic coming from the test user sadiqh
	      
	      kubectl apply -f virtual-service-ratings-test-delay.yaml -n bookinfo-istio
       
       11.4.2 Below is the scenario:
                                       (user:sadiqh)
              reviews:v2 <== http fault injection, 7 sec delay ==> ratings 

              reviews:v2 <==10s hard coded timeout==> ratings 

              productpage <== 3s + 1 retry total 6s ==> reviews
	      
       11.4.3 Testing the delay configuration:
              Open bookinfo app in browser, and login with user sadiqh.
	      we expect the Bookinfo home page to load without errors in approximately 7 seconds, but we are faced with below error.
	      
	      Error fetching product reviews!
              Sorry, product reviews are currently unavailable for this book.
	      
	      View the web page response times:
              Open the Developer Tools menu in you web browser.Open the Network tab and Reload the /productpage web page. 
	      You will see that the page actually loads in about 6 seconds.
	      
	      As expected, the 7s delay you introduced doesnâ€™t affect the reviews service because the timeout between the reviews and ratings service is hard-coded at 10s. 
	      However, there is also a hard-coded timeout between the productpage and the reviews service, coded as 3s + 1 retry for 6s total.
	      As a result, the productpage call to reviews times out prematurely and throws an error after 6s.
	      
      11.4.4 Fixing the bug: Either increasing the productpage to reviews service timeout or decreasing the reviews to ratings timeout
      
 11.5 Injecting an HTTP abort fault:
      
      11.5.1 Test microservice resiliency by injecting an HTTP abort fault. 
             we will introduce an HTTP abort to the ratings microservices for the test user sadiqh.
             In this case, the page to load immediately and display the Ratings service is currently unavailable message.

      11.5.2 Create a fault injection rule to send an HTTP abort for user sadiqh.
             
	     kubectl apply -f virtual-service-ratings-test-abort.yaml -n bookinfo-istio
	     
      11.5.3 Testing the abort configuration:
             Login to bookinfo app with user sadiqh,the page loads immediately and the Ratings service is currently unavailable message appears.
	     
11.6 Mirroring traffic to particular version:
     
     11.6.1 Lets mirror 50% of traffic coming to reviews:v2 to reviews:v3
     
            kubectl apply -f virtual-service-reviews-mirror-v2-v3.yaml -n bookinfo-istio
            
============================================================================================================================================================================================================
============================================================================================================================================================================================================



 
  


